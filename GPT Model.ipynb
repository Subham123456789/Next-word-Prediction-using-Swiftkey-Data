{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "news= open('en_US/en_US.news.txt',encoding=\"utf8\").read()\n",
    "blogs= open('en_US/en_US.blogs.txt',encoding=\"utf8\").read()\n",
    "twitter= open('en_US/en_US.twitter.txt',encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576413590\n"
     ]
    }
   ],
   "source": [
    "text_corpus=news+blogs+twitter \n",
    "print(len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del news , blogs , twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting 1% corpus into sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51608\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "text_corpus = tokenize.sent_tokenize(text_corpus[0:int(len(text_corpus)*0.01)])\n",
    "print(len(text_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51608 sentences are present in 1% of the  corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extra_space(text):\n",
    "    new_text= re.sub(\"\\s+\",\" \",text)\n",
    "    return new_text\n",
    "def sp_charac(text):\n",
    "    new_text=re.sub(\"[^0-9A-Za-z ]\", \"\" , text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing extra space and special characters to find the average word count in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_corpus)) : \n",
    "    text_corpus[i] = extra_space(text_corpus[i])\n",
    "    text_corpus[i] = sp_charac(text_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 51608/51608 [00:00<00:00, 445174.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.69214075337157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count=[]\n",
    "for i in tqdm(text_corpus):\n",
    "    count.append(len(i.split()))\n",
    "print(np.mean(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the sentences in train and test data and saving it in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = text_corpus[0:int(0.8*len(text_corpus))]\n",
    "test_sent = text_corpus[int(0.8*len(text_corpus)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent='.'.join(train_sent)\n",
    "test_sent='.'.join(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_sent.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write(train_sent)\n",
    "with open(\"test_sent.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write(test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling using GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reference : https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b\n",
    "from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "#Calling GPT-1 tokenizer\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 40478, max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size: %d, max sequence length: %d' % (tokenizer.vocab_size, tokenizer.model_max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data collator for language modeling , it will generate batch of train and test data while training\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at \n",
      "Saving features into cached file cached_lm_OpenAIGPTTokenizer_19_train_sent.txt [took 0.067 s]\n",
      "Creating features from dataset file at \n",
      "Saving features into cached file cached_lm_OpenAIGPTTokenizer_19_test_sent.txt [took 0.022 s]\n"
     ]
    }
   ],
   "source": [
    "# TextDataset function converts tokens to ids for each sentence . Here I have set each sentence length to be 19 as the\n",
    "# the average word count is 18.69\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='train_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=19)\n",
    "     \n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='test_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/openai-gpt/resolve/main/config.json from cache at C:\\Users\\ADMIN/.cache\\huggingface\\transformers\\bebb46f5735701bc248ef9faa26f12577944fa7fc8e9be1a774b94d4cb8b79b6.ba6f10a5446f364b92311c09e55e49aa27024a4aeefc1ea50fd733b77bcd997d\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/openai-gpt/resolve/main/pytorch_model.bin from cache at C:\\Users\\ADMIN/.cache\\huggingface\\transformers\\3e867ce638da986403594a5acbb39846ecb9c3b360a3b526348dd54b06938e55.93527980a112896731f93175b7c1cbc6b0fd771fad85fcc777ff5d49d249782e\n",
      "All model checkpoint weights were used when initializing OpenAIGPTLMHeadModel.\n",
      "\n",
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Calling the pretrained GPT-1 Model \n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning of GPT-1 Model on custom data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Setting the arguments for training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'gpt_model', \n",
    "    overwrite_output_dir = True, \n",
    "    per_device_train_batch_size = 64, \n",
    "    per_device_eval_batch_size = 64, \n",
    "    learning_rate = 5e-4, \n",
    "    num_train_epochs = 3,\n",
    ")\n",
    "# Initializing the trainer class object that will do the training\n",
    "# here the data collator will generate the batch of size 64 of train and test data\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 47958\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 12:09:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.968400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt_model\\checkpoint-500\n",
      "Configuration saved in gpt_model\\checkpoint-500\\config.json\n",
      "Model weights saved in gpt_model\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to gpt_model\\checkpoint-1000\n",
      "Configuration saved in gpt_model\\checkpoint-1000\\config.json\n",
      "Model weights saved in gpt_model\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to gpt_model\\checkpoint-1500\n",
      "Configuration saved in gpt_model\\checkpoint-1500\\config.json\n",
      "Model weights saved in gpt_model\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to gpt_model\\checkpoint-2000\n",
      "Configuration saved in gpt_model\\checkpoint-2000\\config.json\n",
      "Model weights saved in gpt_model\\checkpoint-2000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=4.778721299913195, metrics={'train_runtime': 43771.0892, 'train_samples_per_second': 3.287, 'train_steps_per_second': 0.051, 'total_flos': 1911361108506624.0, 'train_loss': 4.778721299913195, 'epoch': 3.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model for 3 epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The train loss achieved is 4.778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt_model\n",
      "Configuration saved in gpt_model\\config.json\n",
      "Model weights saved in gpt_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Saving the model \n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11951\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='187' max='187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [187/187 22:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.522177696228027,\n",
       " 'eval_runtime': 1369.1668,\n",
       " 'eval_samples_per_second': 8.729,\n",
       " 'eval_steps_per_second': 0.137,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating on Test data\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss is 5.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline object for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', tokenizer='openai-gpt', model='gpt_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a beautiful sunset\n",
      "There was a beautiful simplicity\n",
      "There was a beautiful and\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generating  next word in 3 possible ways\n",
    "1. Greedy Search : chooses the best possible next word based on highest probability from 1 hypothesis\n",
    "2. Beam Search : chooses the high probability next word from n hypothesis\n",
    "3. Random Sampling : chooses random next word from possible hypothesis , however as the temperature is set high , it will\n",
    "   ignore low probability words.\n",
    "'''\n",
    "\n",
    "print(generator('There was a beautiful', max_length=5)[0]['generated_text'])\n",
    "print(generator('There was a beautiful', max_length=5,num_beams = 5)[0]['generated_text'])\n",
    "print(generator('There was a beautiful' , max_length=5 , do_sample=True,temperature = 0.7)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Next word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next():\n",
    "    from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,\\\n",
    "    TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling\n",
    "    import re \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    \n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "    generator = pipeline('text-generation', tokenizer='openai-gpt', model='gpt_model') \n",
    "    while(True):\n",
    "        text = input('Enter the text: ')\n",
    "        length= len(tokenizer.encode(text, return_tensors='pt')[0])\n",
    "        \n",
    "        max_length = length+1\n",
    "    \n",
    "        print('Next Word: ')\n",
    "        print(generator(text , max_length=max_length)[0]['generated_text'].split(' ')[-1])\n",
    "        print(generator(text , max_length=max_length , num_beams = 5)[0]['generated_text'].split(' ')[-1])\n",
    "        print(generator(text , max_length=max_length , do_sample=True,temperature = 0.7)[0]['generated_text'].split(' ')[-1])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text: please be aware of the\n",
      "Next Word: \n",
      "situation\n",
      "situation\n",
      "situation\n",
      "Enter the text: Covid-19 is a pandemic which is \n",
      "Next Word: \n",
      "designed\n",
      "used\n",
      "more\n",
      "Enter the text: Indian culture is \n",
      "Next Word: \n",
      "based\n",
      "a\n",
      "in\n",
      "Enter the text: u r looking too\n",
      "Next Word: \n",
      "good\n",
      "good\n",
      "big\n",
      "Enter the text: I am a fan of those\n",
      "Next Word: \n",
      "who\n",
      "buns\n",
      "buns\n",
      "Enter the text: How long do you think\n",
      "Next Word: \n",
      "he\n",
      "this\n",
      "the\n",
      "Enter the text: My impression of \n",
      "Next Word: \n",
      "james\n",
      "the\n",
      "myself\n",
      "Enter the text: He did you a \n",
      "Next Word: \n",
      "wonderful\n",
      "lot\n",
      "lot\n",
      "Enter the text: can I ask for \n",
      "Next Word: \n",
      "your\n",
      "your\n",
      "a\n",
      "Enter the text: Is it a valid medical\n",
      "Next Word: \n",
      "request\n",
      "insurance\n",
      "insurance\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d8c825b63c67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-9ee7ffeb67d8>\u001b[0m in \u001b[0;36mpredict_next\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text-generation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'openai-gpt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gpt_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter the text: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m         )\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "predict_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------+--------------------+\n",
      "|         Model Name        | Train Loss |     Test Loss      |\n",
      "+---------------------------+------------+--------------------+\n",
      "|        Markov Model       |     NA     | 82351 (Perplexity) |\n",
      "|      LSTM_len2 Model      |    6.51    |        7.75        |\n",
      "|      LSTM_len4 Model      |    5.72    |        9.33        |\n",
      "|      LSTM_len7 Model      |    5.78    |       12.14        |\n",
      "| LSTM_len2 Attention Model |    6.56    |        7.09        |\n",
      "| LSTM_len4 Attention Model |    3.50    |        8.86        |\n",
      "| LSTM_len7 Attention Model |    2.61    |        9.24        |\n",
      "|        ALBert Model       |    2.74    |        2.88        |\n",
      "|         GPT Model         |    4.77    |        5.52        |\n",
      "+---------------------------+------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "results = PrettyTable([\"Model Name\", \"Train Loss\", \"Test Loss\"])\n",
    "results.add_row([\"Markov Model\", \"NA\", \"82351 (Perplexity)\"])\n",
    "results.add_row([\"LSTM_len2 Model\", \"6.51\", \"7.75\"])\n",
    "results.add_row([\"LSTM_len4 Model\", \"5.72\", \"9.33\"])\n",
    "results.add_row([\"LSTM_len7 Model\", \"5.78\", \"12.14\"])\n",
    "results.add_row([\"LSTM_len2 Attention Model\", \"6.56\", \"7.09\"])\n",
    "results.add_row([\"LSTM_len4 Attention Model\", \"3.50\", \"8.86\"])\n",
    "results.add_row([\"LSTM_len7 Attention Model\", \"2.61\", \"9.24\"])\n",
    "results.add_row([\"ALBert Model\", \"2.74\", \"2.88\"])\n",
    "results.add_row([\"GPT Model\", \"4.77\", \"5.52\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
