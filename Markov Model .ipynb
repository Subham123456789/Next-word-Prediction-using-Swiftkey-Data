{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARKOV MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import time\n",
    "import orjson\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "with open('uni_dict.json', 'r') as fp:\n",
    "    uni_dict=orjson.loads( fp.read())\n",
    "with open('bi_dict.json', 'r') as fp:\n",
    "    bi_dict=orjson.loads( fp.read())\n",
    "with open('tri_dict.json', 'r') as fp:\n",
    "    tri_dict=orjson.loads( fp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some functions for text cleaning and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_space(text):\n",
    "    new_text= re.sub(\"\\s+\",\" \",text)\n",
    "    return new_text\n",
    "def sp_charac(text):\n",
    "    new_text=re.sub(\"[^0-9A-Za-z ]\", \"\" , text)\n",
    "    return new_text\n",
    "def tokenize_text(text):\n",
    "    new_text=word_tokenize(text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for prediction of next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "def unipred(word): #This function is used when we have only one preceding word \n",
    "    if word not in uni_dict.keys(): #if that word does not exist in our dictionary then we predict some random word\n",
    "        \n",
    "        preds=Counter(uni_dict[random.choice(list(uni_dict.keys()))]).most_common()[:3]\n",
    "        for k,v in preds:\n",
    "            print(k ,':' ,v)\n",
    "    else:\n",
    "        preds=Counter(uni_dict[word]).most_common()[:3]\n",
    "        for k,v in preds:\n",
    "            print(k ,':' ,v)\n",
    "def bipred(word1,word2):#This function is used when we have only 2 preceding words \n",
    "'''\n",
    "if that phrase does not exist in our dictionary then we move to the lower gram to make a prediction\n",
    "'''\n",
    "    if word1+\" \"+word2 not in bi_dict.keys():\n",
    "        unipred(word2)\n",
    "    else:\n",
    "        preds=Counter(bi_dict[word1+\" \"+word2]).most_common()[:3]\n",
    "        for k,v in preds:\n",
    "            print(k ,':' ,v)    \n",
    "def tripred(word1,word2,word3):#This function is used when we have only 3 preceding words \n",
    "'''\n",
    "if that phrase does not exist in our dictionary then we move to the lower gram to make a prediction\n",
    "'''\n",
    "    if word1+\" \"+word2+\" \"+word3 not in tri_dict.keys():\n",
    "        bipred(word2,word3)\n",
    "    else:\n",
    "        preds=Counter(tri_dict[word1+\" \"+word2+\" \"+word3]).most_common()[:3]\n",
    "        for k,v in preds:\n",
    "            print(k ,':' ,v)  \n",
    "def multipred(tokens):#This function is used when we have only 3 preceding words \n",
    "'''\n",
    "We take the last 3 words as history for prediction\n",
    "'''\n",
    "    last_3=tokens[-3:]\n",
    "    tripred(last_3[0],last_3[1],last_3[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicting next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this function we take input , clean that data and convert them into tokens \n",
    "and then depending on the numer of tokens we have we use it to predict the next word\n",
    "This function keeps on running and has to be explicitly stopped.\n",
    "'''\n",
    "def markov_model():\n",
    "    while(True):\n",
    "        text=input()\n",
    "        start=time.time()\n",
    "        cleaned_text=extra_space(text)\n",
    "        cleaned_text=sp_charac(cleaned_text)\n",
    "        tokenized=tokenize_text(cleaned_text)\n",
    "        if len(tokenized)==1:\n",
    "            unipred(tokenized[0])\n",
    "        elif len (tokenized)==2:\n",
    "            bipred(tokenized[0],tokenized[1])\n",
    "        elif len(tokenized)==3:\n",
    "            tripred(tokenized[0],tokenized[1],tokenized[2])\n",
    "        else:\n",
    "            multipred(tokenized)\n",
    "        print('Time Taken: ',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instagrammeer\n",
      "fighters : 2.512413415952653e-06\n",
      "Time Taken:  0.0874943733215332\n",
      "how are\n",
      "you : 0.0012590088941538014\n",
      "u : 0.00013292324978117823\n",
      "we : 6.269964612319728e-05\n",
      "Time Taken:  0.008975028991699219\n",
      "never have i ever\n",
      "had : 1.2559637344658218e-05\n",
      "get : 1.172232818834767e-05\n",
      "did : 1.0047709875726575e-05\n",
      "Time Taken:  0.013724803924560547\n",
      "last tim\n",
      "tebow : 1.2561025649614376e-05\n",
      "and : 5.02441025984575e-06\n",
      "hortons : 5.02441025984575e-06\n",
      "Time Taken:  0.012381315231323242\n",
      "folow\n",
      "me : 1.0886833045390557e-05\n",
      "back : 8.374486957992736e-06\n",
      "let : 1.6748973915985472e-06\n",
      "Time Taken:  0.006621599197387695\n",
      "games should be played\n",
      "Unfortunately : 1.8977402657785242e-06\n",
      "and : 1.8977402657785242e-06\n",
      "This : 1.8977402657785242e-06\n",
      "Time Taken:  0.010994911193847656\n",
      "Germany is\n",
      "the : 6.699426864031782e-06\n",
      "a : 5.861998506027809e-06\n",
      "that : 3.349713432015891e-06\n",
      "Time Taken:  0.008207559585571289\n",
      "German spy \n",
      "by : 1.6749422773017684e-06\n",
      "but : 1.6749422773017684e-06\n",
      "Time Taken:  0.008045196533203125\n",
      "spy \n",
      "on : 8.704320104719666e-05\n",
      "and : 2.42716618304683e-05\n",
      "agency : 1.9249938693130033e-05\n",
      "Time Taken:  0.015990734100341797\n",
      "CIA\n",
      "and : 6.611697515675582e-05\n",
      "agent : 3.096617823797424e-05\n",
      "agents : 1.5064627250906389e-05\n",
      "Time Taken:  0.010993480682373047\n",
      "movies real \n",
      "nature : 1.6749422773017684e-06\n",
      "soon : 1.6749422773017684e-06\n",
      "Time Taken:  0.008717536926269531\n",
      "Hanna\n",
      "and : 2.7627639695392714e-05\n",
      "said : 2.2604432478048584e-05\n",
      "Theatre : 1.0883615637578948e-05\n",
      "Time Taken:  0.012989282608032227\n",
      "Netflix \n",
      "and : 7.110959410643684e-05\n",
      "I : 3.848283916348347e-05\n",
      "is : 3.4299921863104826e-05\n",
      "Time Taken:  0.012989282608032227\n",
      "forever amazon \n",
      "and : 1.5071463857792365e-05\n",
      "prime : 1.0047642571861577e-05\n",
      "I : 7.535731928896182e-06\n",
      "Time Taken:  0.020769119262695312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-69820cc8be5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmarkov_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-8c3188e478ac>\u001b[0m in \u001b[0;36mmarkov_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmarkov_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcleaned_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m         )\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "markov_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
