{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Development markov model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrK-L4uXExA"
      },
      "source": [
        "import nltk\n",
        "from collections import Counter, defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc5-aKVfXExK"
      },
      "source": [
        "ngram_dict1=defaultdict(lambda: defaultdict(lambda: 0))\n",
        "ngram_dict2=defaultdict(lambda: defaultdict(lambda: 0))\n",
        "ngram_dict3=defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBtoCpHy9f6h"
      },
      "source": [
        "## Reading the cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEc4aV31XExL",
        "outputId": "76798805-eff2-4a3f-9256-b759a4b2230a"
      },
      "source": [
        "import pickle\n",
        "with open(\"cleaned_text.txt\", \"rb\") as fp:   # Unpickling\n",
        "    cleaned_text = pickle.load(fp)\n",
        "with open(\"cleaned_twitter.txt\", \"rb\") as fp:   # Unpickling\n",
        "    cleaned_twitter = pickle.load(fp)\n",
        "cleaned_corpus=cleaned_text+cleaned_twitter\n",
        "print(len(cleaned_corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101098262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4GLt5M_9f6j"
      },
      "source": [
        "### Generating unigram dictionaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozqAF1Lb98Pq"
      },
      "source": [
        "code reference : [link](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aDSZmw6XExM"
      },
      "source": [
        "'''\n",
        "Creating a dictionary that has unigram words as the key and the word following it along with \n",
        "its frequency of occurrence with the previous one is the value.\n",
        "'''\n",
        "def gen_uni_keys(cleaned_corpus):\n",
        "    for i in range(len(cleaned_corpus) - 1):\n",
        "        \n",
        "          yield [ cleaned_corpus[i], cleaned_corpus[i + 1] ]\n",
        "uni_pairs=gen_uni_keys(cleaned_corpus)\n",
        "for pair in uni_pairs:\n",
        "    ngram_dict1[pair[0]][pair[1]]+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vJ8_hH_XExN"
      },
      "source": [
        "del uni_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDV7AUtf9f6k"
      },
      "source": [
        "#Saving it as a JSON file\n",
        "import json\n",
        "\n",
        "with open('uni_dict.json', 'w') as fp:\n",
        "    json.dump(ngram_dict1, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LRXgzGQ9f6l"
      },
      "source": [
        "#deleting the unnecessary files\n",
        "del dict_temp\n",
        "del ngram_dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dcDJsv79f6l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNJ1hK_t9f6l"
      },
      "source": [
        "### Generating bigram dictionaries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEUCLofAXExN"
      },
      "source": [
        "'''\n",
        "Creating a dictionary that has bigram words as the key and the word following it along with \n",
        "its frequency of occurrence with the previous one is the value.\n",
        "'''\n",
        "i=0\n",
        "def gen_bi_keys(cleaned_corpus):\n",
        "    for i in range(len(cleaned_corpus) - 2):\n",
        "        \n",
        "          yield [ cleaned_corpus[i], cleaned_corpus[i + 1] ,cleaned_corpus[i+2]]\n",
        "bi_pairs=gen_bi_keys(cleaned_corpus)\n",
        "for pair in bi_pairs:\n",
        "    i=i+1\n",
        "    ngram_dict2[pair[0]+\" \"+pair[1]][pair[2]]+=1\n",
        "    \n",
        "    if i % 2000000==0 :\n",
        "        print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npLZnv_B9f6m"
      },
      "source": [
        "# Saving it in the file\n",
        "import json\n",
        "with open('bi_dict.json', 'w') as fp:\n",
        "    json.dump(ngram_dict2, fp)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2MfK3j9f6n"
      },
      "source": [
        "del bi_pairs\n",
        "del ngram_dict2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PA0UaTk9f6n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ubLhkJe9f6n"
      },
      "source": [
        "### Generating trigram dictionaries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9ZOnzd59f6n",
        "outputId": "617fd91d-d990-443e-a8de-e6506c77ca5c"
      },
      "source": [
        "'''\n",
        "Creating a dictionary that has bigram words as the key and the word following it along with \n",
        "its frequency of occurrence with the previous one is the value.Here I have used only 50000000 tokens because \n",
        "the entire corpus was causing memory error\n",
        "'''\n",
        "i=0\n",
        "def gen_tri_keys(cleaned_corpus):\n",
        "    for i in range(len(cleaned_corpus) - 3):\n",
        "        \n",
        "          yield [ cleaned_corpus[i], cleaned_corpus[i + 1] ,cleaned_corpus[i+2],cleaned_corpus[i+3]]\n",
        "tri_pairs=gen_tri_keys(cleaned_corpus[:50000000])\n",
        "for pair in tri_pairs:\n",
        "    i=i+1\n",
        "    ngram_dict3[pair[0]+\" \" +pair[1]+\" \"+pair[2]][pair[3]]+=1\n",
        "    if i % 2000000 == 0:\n",
        "        print(i)\n",
        "   \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000000\n",
            "4000000\n",
            "6000000\n",
            "8000000\n",
            "10000000\n",
            "12000000\n",
            "14000000\n",
            "16000000\n",
            "18000000\n",
            "20000000\n",
            "22000000\n",
            "24000000\n",
            "26000000\n",
            "28000000\n",
            "30000000\n",
            "32000000\n",
            "34000000\n",
            "36000000\n",
            "38000000\n",
            "40000000\n",
            "42000000\n",
            "44000000\n",
            "46000000\n",
            "48000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWz2paFM9f6o"
      },
      "source": [
        "# saving it in the file\n",
        "import json\n",
        "with open('tri_dict.json', 'w') as fp:\n",
        "    json.dump(ngram_dict3, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdXMXVkf9f6o"
      },
      "source": [
        "del ngram_dict3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NshwiJen9f6p"
      },
      "source": [
        "del tri_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPiKSMo29f6p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-_ssf_99f6p"
      },
      "source": [
        "### Replacing the counts with probability "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKzPRQGc9f6p"
      },
      "source": [
        "### Unigram Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pulyr679f6q"
      },
      "source": [
        "# Reading the unigram dictionary\n",
        "with open('uni_dict.json', 'r') as fp:\n",
        "    ngram_dict1=json.load( fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjcR-oY99f6q",
        "outputId": "b2e2f3a7-8371-4b9f-f8e4-b8a71d453de6"
      },
      "source": [
        "# Calculating vocabulary of the entire corpus\n",
        "vocab=len(set(cleaned_corpus))\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1194069"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HniQLNqD9f6q"
      },
      "source": [
        "'''\n",
        "Here I ave defined the probability function where I calculate the total count of all the words succeeding the key.\n",
        "Then I perform Laplace smoothing for each of the next word by adding 1 to its count \n",
        "and dividing by the sum of total count and vocabulary.\n",
        "'''\n",
        "def prob(ngram_dict):\n",
        "    for words in ngram_dict:\n",
        "        total_count = float(sum(ngram_dict[words].values()))\n",
        "        for nw in ngram_dict[words]:\n",
        "            ngram_dict[words][nw] = (ngram_dict[words][nw]+1)/(total_count+vocab)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByVBlUIt9f6r"
      },
      "source": [
        "#calculating probability\n",
        "prob(ngram_dict1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTvuTQUu9f6r"
      },
      "source": [
        "#Saving the new dictionary back\n",
        "with open('uni_dict.json', 'w') as fp:\n",
        "    json.dump(ngram_dict1, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6hCCns_9f6r"
      },
      "source": [
        "del ngram_dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plOsgxsO9f6s"
      },
      "source": [
        "### Bigram Dictionary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXdd2BTy9f6s"
      },
      "source": [
        "# Doing it for bigram dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL71d1uw9f6t"
      },
      "source": [
        "with open('bi_dict.json', 'r') as fp:\n",
        "    ngram_dict2=json.load( fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHUIJKpr9f6t"
      },
      "source": [
        "prob(ngram_dict2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5UB7eG89f6t"
      },
      "source": [
        "with open('bi_dict.json', 'w') as fp:\n",
        "    json.dump(ngram_dict2, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Twy6jA9f6u"
      },
      "source": [
        "del ngram_dict2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAe-lZqs9f6u"
      },
      "source": [
        "### Trigram Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__H__dJ19f6u"
      },
      "source": [
        "# Doing it for trigram "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZCcXRF09f6u"
      },
      "source": [
        "import json\n",
        "with open('tri_dict.json', 'r') as fp:\n",
        "    ngram_dict3=json.load( fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvbexT99f6v"
      },
      "source": [
        "tri_vocab=len(set(cleaned_corpus[:50000000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-TMbGDm9f6v"
      },
      "source": [
        "'''\n",
        "Defining the function again with the new vocab\n",
        "'''\n",
        "def tri_prob(ngram_dict):\n",
        "    for words in ngram_dict:\n",
        "        total_count = float(sum(ngram_dict[words].values()))\n",
        "        for nw in ngram_dict[words]:\n",
        "            ngram_dict[words][nw] = (ngram_dict[words][nw]+1)/(total_count+tri_vocab)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H4lE0bx9f6v"
      },
      "source": [
        "tri_prob(ngram_dict3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrpXCE3L9f6v"
      },
      "source": [
        "#Saving it back\n",
        "with open('tri_dict.json', 'w') as fp:\n",
        "    json.dump(ngram_dict3, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLY0uwgc9f6w"
      },
      "source": [
        "del ngram_dict3_dict3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jbhQJp-9f6w"
      },
      "source": [
        "del tri_vocab\n",
        "del cleaned_corpus\n",
        "del cleaned_text\n",
        "del cleaned_twitter"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}